{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Programming Assignment \n",
    "\n",
    "Remark: \n",
    "\n",
    "Please upload your solutions of this assignment to Canvas with a file named \"Programming_Assignment_2 _yourname.ipynb\" before 11:59pm May 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l2ENBj8Nz3kg"
   },
   "source": [
    "================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t8Op_NP5z3kf"
   },
   "source": [
    "### **Problem 1 (4 pt).** Use stochastic gradient descent method to train MNIST with the logisitc regression model to achieve at least 92% test accuracy. Print the results with the following format:\n",
    "\n",
    "   \"Epoch: i, Training accuracy: $a_i$, Test accuracy: $b_i$\"\n",
    "\n",
    "where $i=1,2,3,...$ means the $i$-th epoch,  $a_i$ and $b_i$ are the training accuracy and test accuracy computed at the end of $i$-th epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, the training accuracy: 0.88925, the test accuracy: 0.8981\n",
      "Epoch: 2, the training accuracy: 0.9006166666666666, the test accuracy: 0.9078\n",
      "Epoch: 3, the training accuracy: 0.9066333333333333, the test accuracy: 0.9122\n",
      "Epoch: 4, the training accuracy: 0.9104333333333333, the test accuracy: 0.9146\n",
      "Epoch: 5, the training accuracy: 0.9129666666666667, the test accuracy: 0.9167\n",
      "Epoch: 6, the training accuracy: 0.9146166666666666, the test accuracy: 0.918\n",
      "Epoch: 7, the training accuracy: 0.9158166666666666, the test accuracy: 0.9187\n",
      "Epoch: 8, the training accuracy: 0.9172, the test accuracy: 0.9191\n",
      "Epoch: 9, the training accuracy: 0.9181333333333334, the test accuracy: 0.9195\n",
      "Epoch: 10, the training accuracy: 0.9189, the test accuracy: 0.92\n",
      "Epoch: 11, the training accuracy: 0.9196, the test accuracy: 0.9205\n",
      "Epoch: 12, the training accuracy: 0.9202333333333333, the test accuracy: 0.9213\n",
      "Epoch: 13, the training accuracy: 0.9208166666666666, the test accuracy: 0.9215\n",
      "Epoch: 14, the training accuracy: 0.9213833333333333, the test accuracy: 0.9217\n",
      "Epoch: 15, the training accuracy: 0.92175, the test accuracy: 0.9217\n",
      "Epoch: 16, the training accuracy: 0.9220333333333334, the test accuracy: 0.9218\n",
      "Epoch: 17, the training accuracy: 0.92275, the test accuracy: 0.922\n",
      "Epoch: 18, the training accuracy: 0.9230833333333334, the test accuracy: 0.9224\n",
      "Epoch: 19, the training accuracy: 0.9235333333333333, the test accuracy: 0.9229\n",
      "Epoch: 20, the training accuracy: 0.9237666666666666, the test accuracy: 0.9231\n"
     ]
    }
   ],
   "source": [
    "# write your code for solving probelm 1 in this cell\n",
    "# Hint: you can tune the number of epoches, learning rate and mini-batch size.\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def model(input_size,num_classes):\n",
    "    return nn.Linear(input_size,num_classes)\n",
    "\n",
    "\n",
    "# define parameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "batch_size = 128\n",
    "lr = 0.1\n",
    "num_epochs = 20\n",
    "\n",
    "# Step 1: Define a model\n",
    "my_model =model(input_size, num_classes)\n",
    "\n",
    "\n",
    "# Step 2: Define a loss function and training algorithm\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(my_model.parameters(), lr=lr)\n",
    "\n",
    "# Step 3: load dataset\n",
    "MNIST_transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train= True, download=True, transform=MNIST_transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size= batch_size)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train= False, download=True, transform=MNIST_transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1) \n",
    "\n",
    "\n",
    "# Step 4: Train the NNs\n",
    "# One epoch is when an entire dataset is passed through the neural network only once.\n",
    "for epoch in range(num_epochs):\n",
    "    # Train the model\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        \n",
    "        images = images.reshape(images.size(0), 28*28)\n",
    "        \n",
    "        # Forward pass to get the loss\n",
    "        outputs = my_model(images) \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and compute the gradient\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()  \n",
    "        optimizer.step() \n",
    "        \n",
    "    # Training accuracy\n",
    "    # Re-initialize these variables before the for loop\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        images = images.reshape(images.size(0), 28*28) \n",
    "        outputs = my_model(images)\n",
    "        p_max, predicted = torch.max(outputs, 1) \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    # Compute the accuracy after the for loop\n",
    "    training_accuracy = float(correct)/total\n",
    "\n",
    "    \n",
    "    # Test accuracy\n",
    "    # Re-initialize these variables before the for loop\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(testloader):\n",
    "        images = images.reshape(images.size(0), 28*28) \n",
    "        outputs = my_model(images)\n",
    "        p_max, predicted = torch.max(outputs, 1) \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    # Compute the accuracy after the for loop\n",
    "    test_accuracy = float(correct)/total\n",
    "        \n",
    "    # print the results at the end of every epoch (within \"for epoch in range(num_epochs)\")\n",
    "    print('Epoch: {}, the training accuracy: {}, the test accuracy: {}' .format(epoch+1,training_accuracy,test_accuracy))               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l2ENBj8Nz3kg"
   },
   "source": [
    "================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 2 (6 pts).** Extract the subset of data which are labeled with 0,1,3,4,7 from MNIST dataset. Use both full gradient descent method and stochastic gradient descent method to train this subset with the logisitc regression model to achieve the training and test accuracy as high as possible. Print the results with the following format:\n",
    "\n",
    "* For full gradient descent method, print:\n",
    "\n",
    "    \"Full gradient descent, Epoch: i, Training accuracy: $a_i$, Test accuracy: $b_i$\"\n",
    "\n",
    "\n",
    "* For stochastic gradient descent method, print:\n",
    "\n",
    "    \"Stochastic gradient descent, Epoch: i, Training accuracy: $a_i$, Test accuracy: $b_i$\"\n",
    "\n",
    "where $i=1,2,3,...$ means the $i$-th epoch,  $a_i$ and $b_i$ are the training accuracy and test accuracy computed at the end of $i$-th epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full gradient descent, Epoch: 1, the training accuracy: 0.8036760185095299, the test accuracy: 0.8177215189873418\n",
      "Full gradient descent, Epoch: 2, the training accuracy: 0.6022392647962981, the test accuracy: 0.6007789678675755\n",
      "Full gradient descent, Epoch: 3, the training accuracy: 0.579781898197586, the test accuracy: 0.5873417721518988\n",
      "Full gradient descent, Epoch: 4, the training accuracy: 0.7050771769731095, the test accuracy: 0.7030185004868549\n",
      "Full gradient descent, Epoch: 5, the training accuracy: 0.7998252596835259, the test accuracy: 0.8023369036027264\n",
      "Full gradient descent, Epoch: 6, the training accuracy: 0.9119826554056241, the test accuracy: 0.916066212268744\n",
      "Full gradient descent, Epoch: 7, the training accuracy: 0.9448920816749183, the test accuracy: 0.9554040895813047\n",
      "Full gradient descent, Epoch: 8, the training accuracy: 0.9502637284406045, the test accuracy: 0.959493670886076\n",
      "Full gradient descent, Epoch: 9, the training accuracy: 0.9518816943338835, the test accuracy: 0.9606621226874391\n",
      "Full gradient descent, Epoch: 10, the training accuracy: 0.9536938161343559, the test accuracy: 0.9620253164556962\n",
      "Full gradient descent, Epoch: 11, the training accuracy: 0.955376500663366, the test accuracy: 0.9626095423563777\n",
      "Full gradient descent, Epoch: 12, the training accuracy: 0.9562825615636023, the test accuracy: 0.9639727361246349\n",
      "Full gradient descent, Epoch: 13, the training accuracy: 0.9574474970067631, the test accuracy: 0.9645569620253165\n",
      "Full gradient descent, Epoch: 14, the training accuracy: 0.9582564799534026, the test accuracy: 0.9659201557935735\n",
      "Full gradient descent, Epoch: 15, the training accuracy: 0.9590007442643109, the test accuracy: 0.9674780915287244\n",
      "Full gradient descent, Epoch: 16, the training accuracy: 0.9598097272109504, the test accuracy: 0.9680623174294061\n",
      "Full gradient descent, Epoch: 17, the training accuracy: 0.9607157881111866, the test accuracy: 0.9682570593962999\n",
      "Full gradient descent, Epoch: 18, the training accuracy: 0.9610393812898425, the test accuracy: 0.9690360272638754\n",
      "Full gradient descent, Epoch: 19, the training accuracy: 0.9614924117399605, the test accuracy: 0.9696202531645569\n",
      "Full gradient descent, Epoch: 20, the training accuracy: 0.9619454421900786, the test accuracy: 0.9700097370983447\n"
     ]
    }
   ],
   "source": [
    "# write your code for solving probelm 2 in this cell\n",
    "# Hint: you can tune the number of epoches, learning rate and mini-batch size.\n",
    "\n",
    "\n",
    "# Full gradient descent method\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def model(input_size,num_classes):\n",
    "    return nn.Linear(input_size,num_classes)\n",
    "\n",
    "\n",
    "#Extract the subset of data which are labeled by 0,1,3,4,7, and re-label them to 0,1,2,3,4\n",
    "def get_indices(dataset):\n",
    "    indices =  []\n",
    "    for i in range(len(dataset.targets)):\n",
    "        if (dataset.targets[i] == 0):\n",
    "            indices.append(i)\n",
    "        if (dataset.targets[i] == 1):\n",
    "            indices.append(i)\n",
    "        if (dataset.targets[i] == 3):\n",
    "            indices.append(i)\n",
    "        if (dataset.targets[i] == 4):\n",
    "            indices.append(i)\n",
    "        if (dataset.targets[i] == 7):\n",
    "            dataset.targets[i] = 2\n",
    "            indices.append(i)  \n",
    "    return indices\n",
    "\n",
    "# define parameters\n",
    "input_size = 784\n",
    "num_classes = 5\n",
    "#batch_size = 128    batch_size should be len(train_idx) which is defined later.\n",
    "lr = 1\n",
    "num_epochs = 20\n",
    "\n",
    "# Step 1: Define a model\n",
    "my_model =model(input_size, num_classes)\n",
    "\n",
    "\n",
    "# Step 2: Define a loss function and training algorithm\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(my_model.parameters(), lr=lr)\n",
    "\n",
    "# Step 3: load dataset\n",
    "MNIST_transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train= True, download=True, transform=MNIST_transform)\n",
    "train_idx = get_indices(trainset)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=len(train_idx), sampler = torch.utils.data.sampler.SubsetRandomSampler(train_idx))\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train= False, download=True, transform=MNIST_transform)\n",
    "test_idx = get_indices(testset)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, sampler = torch.utils.data.sampler.SubsetRandomSampler(test_idx)) \n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Train the NNs\n",
    "# One epoch is when an entire dataset is passed through the neural network only once.\n",
    "for epoch in range(num_epochs):\n",
    "    # Train the model\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        \n",
    "        images = images.reshape(images.size(0), 28*28)\n",
    "        \n",
    "        # Forward pass to get the loss\n",
    "        outputs = my_model(images) \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and compute the gradient\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()  \n",
    "        optimizer.step() \n",
    "        \n",
    "    # Training accuracy\n",
    "    # Re-initialize these variables before the for loop\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        images = images.reshape(images.size(0), 28*28) \n",
    "        outputs = my_model(images)\n",
    "        p_max, predicted = torch.max(outputs, 1) \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    # Compute the accuracy after the for loop\n",
    "    training_accuracy = float(correct)/total\n",
    "\n",
    "    \n",
    "    # Test accuracy\n",
    "    # Re-initialize these variables before the for loop\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(testloader):\n",
    "        images = images.reshape(images.size(0), 28*28) \n",
    "        outputs = my_model(images)\n",
    "        p_max, predicted = torch.max(outputs, 1) \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "        \n",
    "    # Compute the accuracy after the for loop\n",
    "    test_accuracy = float(correct)/total\n",
    "        \n",
    "    # print the results at the end of every epoch (within \"for epoch in range(num_epochs)\")\n",
    "    print('Full gradient descent, Epoch: {}, the training accuracy: {}, the test accuracy: {}' .format(epoch+1,training_accuracy,test_accuracy))               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic gradient descent, Epoch: 1, the training accuracy: 0.9663463094197974, the test accuracy: 0.9731256085686465\n",
      "Stochastic gradient descent, Epoch: 2, the training accuracy: 0.9707795359673818, the test accuracy: 0.9770204479065239\n",
      "Stochastic gradient descent, Epoch: 3, the training accuracy: 0.9731741254894347, the test accuracy: 0.9779941577409932\n",
      "Stochastic gradient descent, Epoch: 4, the training accuracy: 0.9751804031971006, the test accuracy: 0.9801363193768257\n",
      "Stochastic gradient descent, Epoch: 5, the training accuracy: 0.9765394945474549, the test accuracy: 0.9801363193768257\n",
      "Stochastic gradient descent, Epoch: 6, the training accuracy: 0.9773161181762289, the test accuracy: 0.9805258033106135\n",
      "Stochastic gradient descent, Epoch: 7, the training accuracy: 0.978222179076465, the test accuracy: 0.9811100292112951\n",
      "Stochastic gradient descent, Epoch: 8, the training accuracy: 0.9789017247516423, the test accuracy: 0.9807205452775073\n",
      "Stochastic gradient descent, Epoch: 9, the training accuracy: 0.9787399281623144, the test accuracy: 0.9809152872444011\n",
      "Stochastic gradient descent, Epoch: 10, the training accuracy: 0.9795812704268194, the test accuracy: 0.9818889970788705\n",
      "Stochastic gradient descent, Epoch: 11, the training accuracy: 0.9795489111089538, the test accuracy: 0.9814995131450828\n",
      "Stochastic gradient descent, Epoch: 12, the training accuracy: 0.9798077856518784, the test accuracy: 0.9814995131450828\n",
      "Stochastic gradient descent, Epoch: 13, the training accuracy: 0.9801960974662655, the test accuracy: 0.9822784810126582\n",
      "Stochastic gradient descent, Epoch: 14, the training accuracy: 0.9809080024593082, the test accuracy: 0.9820837390457644\n",
      "Stochastic gradient descent, Epoch: 15, the training accuracy: 0.9809403617771737, the test accuracy: 0.9822784810126582\n",
      "Stochastic gradient descent, Epoch: 16, the training accuracy: 0.9810374397307705, the test accuracy: 0.9822784810126582\n",
      "Stochastic gradient descent, Epoch: 17, the training accuracy: 0.9814257515451574, the test accuracy: 0.9826679649464459\n",
      "Stochastic gradient descent, Epoch: 18, the training accuracy: 0.9814257515451574, the test accuracy: 0.9830574488802337\n",
      "Stochastic gradient descent, Epoch: 19, the training accuracy: 0.9820405785846035, the test accuracy: 0.9830574488802337\n",
      "Stochastic gradient descent, Epoch: 20, the training accuracy: 0.9819111413131412, the test accuracy: 0.9828627069133399\n"
     ]
    }
   ],
   "source": [
    "# write your code for solving probelm 2 in this cell\n",
    "# Hint: you can tune the number of epoches, learning rate and mini-batch size.\n",
    "\n",
    "\n",
    "# Stochastic gradient descent method\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def model(input_size,num_classes):\n",
    "    return nn.Linear(input_size,num_classes)\n",
    "\n",
    "\n",
    "#Extract the subset of data which are labeled by 0,1,3,4,7, and re-label them to 0,1,2,3,4\n",
    "def get_indices(dataset):\n",
    "    indices =  []\n",
    "    for i in range(len(dataset.targets)):\n",
    "        if (dataset.targets[i] == 0):\n",
    "            indices.append(i)\n",
    "        if (dataset.targets[i] == 1):\n",
    "            indices.append(i)\n",
    "        if (dataset.targets[i] == 3):\n",
    "            indices.append(i)\n",
    "        if (dataset.targets[i] == 4):\n",
    "            indices.append(i)\n",
    "        if (dataset.targets[i] == 7):\n",
    "            dataset.targets[i] = 2\n",
    "            indices.append(i)  \n",
    "    return indices\n",
    "\n",
    "# define parameters\n",
    "input_size = 784\n",
    "num_classes = 5\n",
    "batch_size = 128\n",
    "lr = 0.1\n",
    "num_epochs = 20\n",
    "\n",
    "# Step 1: Define a model\n",
    "my_model =model(input_size, num_classes)\n",
    "\n",
    "\n",
    "# Step 2: Define a loss function and training algorithm\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(my_model.parameters(), lr=lr)\n",
    "\n",
    "# Step 3: load dataset\n",
    "MNIST_transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train= True, download=True, transform=MNIST_transform)\n",
    "train_idx = get_indices(trainset)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler = torch.utils.data.sampler.SubsetRandomSampler(train_idx))\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train= False, download=True, transform=MNIST_transform)\n",
    "test_idx = get_indices(testset)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, sampler = torch.utils.data.sampler.SubsetRandomSampler(test_idx)) \n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Train the NNs\n",
    "# One epoch is when an entire dataset is passed through the neural network only once.\n",
    "for epoch in range(num_epochs):\n",
    "    # Train the model\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        \n",
    "        images = images.reshape(images.size(0), 28*28)\n",
    "        \n",
    "        # Forward pass to get the loss\n",
    "        outputs = my_model(images) \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and compute the gradient\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()  \n",
    "        optimizer.step() \n",
    "        \n",
    "    # Training accuracy\n",
    "    # Re-initialize these variables before the for loop\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        images = images.reshape(images.size(0), 28*28) \n",
    "        outputs = my_model(images)\n",
    "        p_max, predicted = torch.max(outputs, 1) \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    # Compute the accuracy after the for loop\n",
    "    training_accuracy = float(correct)/total\n",
    "\n",
    "    \n",
    "    # Test accuracy\n",
    "    # Re-initialize these variables before the for loop\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(testloader):\n",
    "        images = images.reshape(images.size(0), 28*28) \n",
    "        outputs = my_model(images)\n",
    "        p_max, predicted = torch.max(outputs, 1) \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    # Compute the accuracy after the for loop\n",
    "    test_accuracy = float(correct)/total\n",
    "        \n",
    "    # print the results at the end of every epoch (within \"for epoch in range(num_epochs)\")\n",
    "    print('Stochastic gradient descent, Epoch: {}, the training accuracy: {}, the test accuracy: {}' .format(epoch+1,training_accuracy,test_accuracy))               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================================================================================================="
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "A8btbBuFz3j_",
    "8tnDF8N5z3j_",
    "6NGxntKLz3kO",
    "t8Op_NP5z3kf",
    "eY74kXCSz3kg",
    "PsX9sU1Nz3kh",
    "tt6oay6Rz3kk",
    "kTHHlHSbz3kn",
    "O-OyyHliz3ko",
    "cL_XkTgmz3k1",
    "QFdILTiHz3k1",
    "XmhEkm0nz3k2",
    "bn4Pbjwgz3k4",
    "MJn6VYpQz3k8",
    "JOcri38sz3k8"
   ],
   "name": "Lecture1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
