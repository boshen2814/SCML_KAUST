{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c5d0eb",
   "metadata": {},
   "source": [
    "# Build and train a logistic regression model with MNIST dataset \n",
    "\n",
    "## Table of contents\n",
    "1. [MNIST database](#MNIST)\n",
    "2. [Dataset, loss, and accuracy ](#data)\n",
    "3. [Build and train a logistic regression model with MNIST dataset](#logre)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>\n",
    "\n",
    "### 1. MNIST database  <a name=\"MNIST\"></a>\n",
    "\n",
    "</b></div>\n",
    "\n",
    "The MNIST database (Modified National Institute of Standards and Technology database) is a large collection of handwritten digits. It has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger NIST Special Database 3 (digits written by employees of the United States Census Bureau) and Special Database 1 (digits written by high school students) which contain monochrome images of handwritten digits. The digits have been size-normalized and centered in a fixed-size image. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.\n",
    "\n",
    "Introduced by [LeCun et al. in Gradient-based learning applied to document recognition](https://arxiv.org/pdf/1102.0183.pdf)\n",
    "\n",
    "Source: http://yann.lecun.com/exdb/mnist/ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79233198",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>\n",
    "\n",
    "### 2. Dataset, loss, and accuracy  <a name=\"data\"></a>\n",
    "\n",
    "</b></div>\n",
    "\n",
    "### Training dataset and test dataset\n",
    "Define a dataset as $D = \\{(x_j, y_j)\\}$ and $D=D_1 \\cup D_2$, where:\n",
    "* $D_1$ is the training dataset and $|D_1|$ is the number of data in $D_1$.\n",
    "* $D_2$ is the test dataset and $|D_2|$ is the number of data in $D_2$.\n",
    "\n",
    "\n",
    "### Training loss and test loss\n",
    "Define the loss function:\n",
    "$$L(\\theta) :=\\frac{1}{N} \\sum_{j=1}^N\\ell(y_j, h(x_j; \\theta)).$$\n",
    "Here $\\ell(y_j,h(x_j; \\theta))$ is the  general distance between real label and predicted label. $h(x_j; \\theta)$ is a probability distribution of data $x$.\n",
    "* Training loss is defined as $L(\\theta) :=\\frac{1}{|D_1|} \\sum_{j=1}^{|D_1|}\\ell(y_j, h(x_j; \\theta)).$\n",
    "* Test loss is defined as $L(\\theta) :=\\frac{1}{|D_2|} \\sum_{j=1}^{|D_2|}\\ell(y_j, h(x_j; \\theta)).$\n",
    "\n",
    "### Training accuracy and test accuracy\n",
    "* Training accuracy $= \\frac{\\text{The number of correct classifications in training dataset}}{\\text{the total number of data in training dataset}}$\n",
    "* Test accuracy $= \\frac{\\text{The number of correct classifications in test dataset}}{\\text{the total number of data in test dataset}}$\n",
    "\n",
    "Remark: We usually use the max-out method to do classification. For a given data point $x$, we first compute $h(x;\\theta)$, then we attached $x$ to the class $i= \\arg\\max_j h_j(x; \\theta)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c22391",
   "metadata": {},
   "source": [
    "### Epoch vs Batch Size vs Iterations\n",
    "\n",
    "When the data is too big which happens all the time in machine learning and we canâ€™t pass all the data to the machine at once. To overcome this problem we need to divide the data into smaller sizes and update the weights of the neural networks at the end of every step to fit it to the data given.\n",
    "\n",
    "\n",
    "#### Epoch\n",
    "\n",
    "One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only once. Since one epoch is too big to feed to the computer at once we divide it in several smaller batches.\n",
    "\n",
    "#### Batch\n",
    "\n",
    "Total number of training examples present in a single batch.\n",
    "\n",
    "#### Iteration\n",
    "\n",
    "Iteration is the number of batches needed to complete one epoch.\n",
    "\n",
    "\n",
    "When the model goes through the whole 60k images once,learning how to classify 0-9, it's consider 1 epoch. However, there's a concept of batch size where it means the model would train with 100 images everytime. When the model updates its weights (parameters) after looking at all the images, this is considered 1 iteration. We arbitrarily set 3000 iterations here which means the model would update 3000 times. \n",
    "\n",
    "One epoch consists of 60,000 / 100 = 600 iterations. Because we would like to go through 3000 iterations, this implies we would have 3000 / 600 = 5 epochs as each epoch has 600 iterations.  \n",
    "\n",
    "\n",
    "total data : 60000\n",
    "\n",
    "batch size: 100 (number of examples in 1 iteration)\n",
    "\n",
    "iterations: 3000 (one batch forward & backward pass)\n",
    "\n",
    "epochs = iterations / (total data/ minibatch)\n",
    "       = 3000/(60000/100) \n",
    "       = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d094bce8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>\n",
    "\n",
    "### 3. Build and train a logistic regression model with MNIST dataset  <a name=\"logre\"></a>\n",
    "\n",
    "</b></div> \n",
    "\n",
    "We train and test a logistic regression model with MNIST dataset. This dataset contains 6000 images for training and 10000 images for testing the out-of-sample performance.\n",
    "\n",
    "### Steps\n",
    "* Step 1: Load MNIST Train and test Dataset\n",
    "* Step 2: Load Dataset into DataLoader\n",
    "* Step 3: Build Model with nn.Module\n",
    "* Step 4: Instantiate Model Class\n",
    "* Step 5: Instantiate Loss Class\n",
    "* Step 6: Instantiate Optimizer Class\n",
    "* Step 7: Train Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
